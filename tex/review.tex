\section{Mathematical Context and Background}
\label{sec:review}

In order to provide some mathematical context for our covariance model
developments, we first outline the generic inverse problem that is central to
a variety of applications such as numerical weather prediction and state
estimation.
Our notation closely follows \citet{ide_unified_1997}, and we note that matrix
notation is used to describe the problem, but these matrices are never formed
explicitly.
Rather, all matrices presented can be described as operators that can be applied
scalably in high dimensional inverse problems.
We then review how the WC01 correlation model fits into the background state
error covariance.
Finally, we discuss the developments from \citet{RSSB:RSSB777}, outlining the
connection between the solution to a Stochastic PDE (SPDE) and a Gaussian random
field with Mat\'ern type covariance.
This review sets up our development of an anistropic, nonstationary covariance
model which is presented in Section \ref{sec:matern_operator}.


\subsection{Inverse Problem Formulation}
\label{ssec:da_formulation}

We consider the general problem of finding the optimal control vector,
$\params$, which minimizes the regularized model-data misfit cost function
\begin{linenomath*}\begin{equation*}
    \cf(\params) =
        \dfrac{1}{2}||\pto(\params) - \data||_{\obsCovMat^{-1}}^2
        +
        \dfrac{1}{2}||\params - \priorParams||_{\priorCovMat^{-1}}^2 \, .
\end{equation*}\end{linenomath*}
The solution to this inverse problem, $\paramsMAP$, arises from a tradeoff between fitting the
observational data, $\data$, as governed by the parameter to observable map
$\pto(\cdot)$, and minimizing deviations from the background-state $\priorParams$.
This tradeoff is governed by the two error covariances, $\obsCovMat$ and
$\priorCovMat$, which dictate how much deviation is acceptable in either term.
On the one hand, the observational error covariance matrix
$\obsCovMat$ represents our uncertainty
in the observational data, together with our confidence in the model's ability
to represent the observed values.
On the other hand, the background-state covariance matrix $\priorCovMat$
represents our uncertainty in the prior estimate or background state,
$\priorParams$.

In this work, our focus is on the development of the background-state error
covariance, $\priorCovMat$.
In the general case, the control vector $\params$ could be multivariate,
including initial conditions of the system state, uncertain boundary
conditions, or uncertain parameter fields.
Here we employ the decomposition proposed by
\citet{derber_reformulation_1999}
in order to separate the multivariate (i.e.\ cross-variable)
covariance relationships from the univariate (i.e.\ assumed
independent) covariance relationships.
Specifically, the background-state covariance is decomposed as follows
\begin{linenomath*}\begin{equation*}
    \priorCovMat \coloneqq \balanceOperator\unbalancedPriorCovMat\balanceOperator^T \,
    ,
\end{equation*}\end{linenomath*}
where $\balanceOperator$ is a balance operator that deals with the
cross-variable correlations.
The matrix $\unbalancedPriorCovMat$ describes the covariance for the unbalanced
variables and has a block-diagonal structure, such that each
covariance is described independently.
The unbalanced covariance is further factored as
\begin{linenomath*}\begin{equation*}
    \unbalancedPriorCovMat \coloneqq \Sigma \corrMat \Sigma
\end{equation*}\end{linenomath*}
where $\Sigma$ is a diagonal scaling matrix, containing the desired pointwise
standard deviation values and $\corrMat$ is a block diagonal correlation matrix,
describing each variable's independent correlation structure.
To be concrete, this can be viewed as
\begin{linenomath*}\begin{equation*}
    \unbalancedPriorCovMat =
    \begin{pmatrix}
        \Sigma_\alpha\corrMat_\alpha\Sigma_\alpha & 0 & \cdots & 0 \\
        0 & \Sigma_\beta\corrMat_\beta\Sigma_\beta & \cdots & 0 \\
        0 & 0 & \ddots & 0  \\
        0 & 0 & \cdots & \Sigma_\gamma\corrMat_\gamma\Sigma_\gamma \\
    \end{pmatrix}
\end{equation*}\end{linenomath*}
where $\alpha, \beta, \gamma$ are placeholders for unique variables, and each
$\Sigma_\alpha, \corrMat_\alpha$ pair describes the amplitude of pointwise standard deviation
and spatial correlation for each variable.


\subsection{The WC01 Correlation Model}
\label{ssec:wc01_review}


The primary contribution of WC01 is a correlation model that can be used to
formulate $\corrMat_\alpha$.
The operator is based on the solution of a generalized diffusion equation,
amounting to the repeated application of a Laplacian-like operator.
The operator can generally be written as
\begin{linenomath}\begin{equation}
    \corrMat_\text{WC01} = \left(
    I + \nabla\cdot\kappa\nabla \right)^p W^{-1/2} \,
\end{equation}\end{linenomath}
where $W \coloneqq \text{diag}\{V_i\}_{i=1}^{\nuni}$ is a diagonal matrix of
grid cell volume elements.
The tensor, $\kappa$, controls the correlation length scales imposed on the
random field in question.
Correlation length scales are isotropic (i.e.\ the same in all directions) and
stationary (i.e.\ they do not change with spatial location) when $\kappa$ is
constant such that
\begin{linenomath}\begin{equation}
    L^2 = 2\kappa T \, ,
\end{equation}\end{linenomath}
where $T\coloneqq p \Delta t$ is the total ``diffusion'' time and $L$ is the
desired length scale.
In general, the diffusion tensor can be formulated to produce anisotropic and
nonstationary length scales, and applied to spherical domains.
However, a few subtle issues arise in the numerical its implementation.
First, the number of timesteps, $p$, required to integrate the diffusion
equation for numerical stability is somewhat unclear.
In WC01, a necessary but insufficient stability requirement
$p > 2 (L/\Delta x)^2$ is given.
However, in practice uncovering the true required value of $p$ requires some
guess work, and in our experience is
\red{a factor of $\sim 3$} of this lower bound.
\red{Note Master's thesis from MIT on variable diffusion tensor stability?}
Additionally, adjoint-based data assimilation systems based on algorithmic
differentiation may require additional storage (or RAM) due to the
pseudo-timestepping nature of the diffusion approach.

In this work we formulate an operator for $\corrMat_\alpha$ which bypasses these
issues entirely.
First, we avoid the issue of ``guessing'' the required time steps for numerical
stability because application of our correlation operator amounts to solving an
elliptic PDE, for which a tolerance can be prescribed.
Moreover, we show that for practical applications this tolerance can be reliably
prescribed as being quite high $\bigo(10^{-2})$, such that solving the required
elliptic PDE can be very efficient.
Finally, the operator is self-adjoint and so implementing the covariance model
within any adjoint based solver is a nonissue.
To provide some details, we next outline the basis for this Mat\'ern covariance
model from \citet{RSSB:RSSB777}.


\subsection{Review of the Mat\'ern class covariance}
\label{ssec:matern_review}

In this section we review the link between an elliptic stochastic partial
differential equation (SPDE) and Gaussian random fields.
The Mat\'ern covariance function between two points, $\xh_1,\xh_2\in\defdomain =
\ndspace$ can be expressed as:
\begin{linenomath}\begin{equation}
    c(\xh_1,\xh_2) = \dfrac{\sigma^2}{2^{\meandiff-1}
    \mathcal{G}(\meandiff)}
    \Big(\sqrt{\deltah} ||\xh_2-\xh_1||\Big)^\meandiff
    \mathcal{B}_\meandiff
    \Big(\sqrt{\deltah} ||\xh_2-\xh_1||\Big) \, .
    \label{eq:matern_covariance_iso}
\end{equation}\end{linenomath}
Here $||\cdot||$ as the Euclidean norm in $\defdomain$,
$\mathcal{G}$ is the Gamma function,
$\mathcal{B}_\meandiff$ is the modified
Bessel function of the second kind and order $\meandiff$,
$\sigma^2$ is the
marginal variance, $\deltah>0$ is a scaling parameter, and $\meandiff>0$
controls the mean-square differentiability of the underlying statistical process
described by the Mat\'ern covariance.
The reason for defining symbols with a hat ($\hat{\cdot}$), will become clear
in the next subsection.
Throughout, we refer to a ``Mat\'ern field'' as any Gaussian field that has
covariance that can be described by the Mat\'ern covariance function,
equation (\ref{eq:matern_covariance_iso}).

The key relationship discussed in \citet{RSSB:RSSB777} is that any Mat\'ern field,
$\unih(\xh)$, is a solution to the elliptic SPDE:
\begin{linenomath}\begin{equation}
    \Big(\deltah - \nablah\cdot\nablah\Big)^{\spdesqo/2}\hat{\uni}(\xh) =
    \Wh(\xh) \, .
    \label{eq:spde_iso}
\end{equation}\end{linenomath}
Here $\spdesqo = \meandiff + \materndim/2$,
$\Wh$ is a white noise process defined on the space $\defdomain$.
We note that the Mat\'ern covariance function describes covariances that are
stationary and isotropic.
That is, stationarity implies that correlation length scales are determined
purely by the Euclidean distance between two points and this does not change as
a function of location in the domain.
Isotropy implies that correlation lengths are the same for the same Euclidean
distance along any dimension.

The original connection between the Mat\'ern covariance function and solutions
to equation (\ref{eq:spde_iso}) was proven by
\cite{whittle_stationary_1954,whittle1963stochastic}, who
used the spectral properties of the operator $(\deltah -
\nablah\cdot\nablah)^{\spdesqo/2}$ to show that Mat\'ern fields are the only
stationary solutions to equation (\ref{eq:spde_iso}).
The result shown in \citet{RSSB:RSSB777} is
that there is an explicit link between discrete solutions to equation
(\ref{eq:spde_iso}) for any triangulation or rectangular lattice of $\ndspace$
and Mat\'ern class Gaussian fields.
The punch line is that we can use all of the computational tools for solving discretized
elliptic equations to apply a covariance operator that is formally dense.
More importantly, \citet{RSSB:RSSB777} showed that the SPDE form allows
one to easily describe Gaussian fields with more general covariance structures.
For instance, by allowing the parameter $\deltah$ to vary in space, the solution
becomes nonstationary and the Mat\'ern covariance applies locally.

In \citet{RSSB:RSSB777} it is suggested that solving the SPDE in a transformed
coordinate system can allow for a Mat\'ern class covariance
model that can easily incorporate anisotropy and nonstationarity.
Consider the isotropic and stationary case, described by equation
(\ref{eq:spde_iso}).
The field $\unih(\xh)$ is defined in a transformed, or ``deformed''
\citep{sampson_nonparametric_1992}, space $\defdomain$.
Assume that we have a mapping $\defmap$ that maps between this transformed space
and our computational domain, $\domain$:
\begin{linenomath*}\begin{equation*}
    \defmap : \defdomain\ni\xh \rightarrow \x\in\domain \, .
\end{equation*}\end{linenomath*}
With this mapping, we can employ a change of variables
\citep{smith_change_1934} to rewrite the SPDE in the computational domain as:
\begin{linenomath*}\begin{equation*}
    \dfrac{1}{\defdet}
    \left(\deltah -
    \defdet\nabla\cdot
    \dfrac{\defjac(\x)\defjac(\x)^T}{\defdet}
    \nabla\right)\uni(\x) =
    \defdet^{-1/2}\W(\x) \, .
\end{equation*}\end{linenomath*}
Here we have defined the Jacobian as
\begin{linenomath*}\begin{equation*}
    \defjac(\x_0) \coloneqq
    \dfrac{\partial \defmap}{\partial \xh}\Big|_{\defmap^{-1}(\x_{0})} \, ,
\end{equation*}\end{linenomath*}
and for now we assume that $\defmap^{-1}(\x_0)$ is well defined.
For our purposes, this turns out to be the case, but this becomes clear when
$\defjac$ is defined in section \ref{sec:matern_operator}.
Notice that we have taken the exponent $\spdesqo/2$ to be 1, avoiding
fractional or higher order operations for simplicity.
All of the future formulations and experiments will make this assumption,
although this can be relaxed in future work.
With the following definitions:
\begin{linenomath}\begin{equation}
    K(\x) \coloneqq
    \dfrac{\defjac(\x)\defjac(\x)^T}{\defdet}
    \qquad
    \delta(\x) \coloneqq \dfrac{\deltah}{\defdet}
    \label{eq:matern_definitions}
\end{equation}\end{linenomath}
the SPDE in the computational domain's coordinate system can be written as
\begin{linenomath}\begin{equation}
    \Big(\delta(\x)- \nabla\cdot K(\x)\nabla\Big)\uni(\x) =
    \defdet^{-1/2}\W(\x) \, .
    \label{eq:spde_general}
\end{equation}\end{linenomath}
We note as in \citet{RSSB:RSSB777} that this reproduces the deformation method
introduced in \citet{sampson_nonparametric_1992}.
The key feature of this formulation is that the deformation map, $\defmap$, is
not actually necessary, only its Jacobian.
The question then becomes, how does one specify $\defjac$ and $\deltah$?
This is the primary question we wish to address.
However, we first develop the discretized form of this SPDE
that is relevant to the finite volume grid of our computational model, the
MITgcm.
